---
title: "Generate synthetic data with ellmer"
format: html
---

Recent packages like ellmer and chatlas make it easy to make your own LLM tools. You can use ellmer and chatlas to interact with common LLM APIs, including OpenAI, Claude, and Gemini.

One use case for these tools is simulating data. There are a lot of things you want want to simulate data for:

* Teaching
* Package or app testing 
* Prototyping a report 
* Clinical trial simulations


Use cases
Generating data for teaching
Generating data for simulation
Generating data for testing
Generating data for a prototype
Data generation is a great use case of the LLM APIs
You might be familiar with using the chat interface of the popular LLMs, but are curious about how to work with their APIs
And why would you want to use their APIs?
One reason is if the LLM can’t do what you want, but you think it probably would be able to
We’ll take advantage of tool calling 
For more, see the 

https://www.confident-ai.com/blog/the-definitive-guide-to-synthetic-data-generation-using-llms

Create a command line data generation tool with elmer

## 

First, let's see how to use the native LLM capability to generate data. The common LLM providers will simulate data for you, which you might have encountered through their chat interface. This capability is really useful, and some LLM's will even let you easily download that data as a CSV, but it's then still an extra step to pull it into R or Python. You might also want to generate the dataset through a Shiny app, which would require using the LLM API. 

Let's build a small tool that uses the LLM to simulate some data. 

## Setup

First, we need to download the necessary packages and set up API keys. You'll need:

* [ellmer](https://ellmer.tidyverse.org/), which simplifies the process of interacting with LLMs from R, and
* [dotenv](https://github.com/gaborcsardi/dotenv), which will simplify the process of using our API keys from your R environment.

```r
install.packages(c("ellmer", "dotenv"))
```

Next, add your API key(s) to your `.Renviron` file. You can choose the LLM that you want to work with. ellmer includes functions for working with OpenAI, Anthropic, Gemini, and other providers. You can see a full list of supported providers [here](https://ellmer.tidyverse.org/#providers). 

Add your desired API key(s) to your `.Renviron`, for example:

```
OPENAI_API_KEY=my-api-key-openai-uejkK92
ANTHROPIC_API_KEY=api-key-anthropic-nxue0
GOOGLE_API_KEY=api-key-google-palw2n
```

## Creat a basic chat with ellmer

Now, we'll create a simple chat with ellmer. 

First, we use a `chat_*()` function to create a `Chat` object. Choose your `chat_*()` function based on your desired LLM provider. 

```{r}
library(ellmer)
library(dotenv)

chat <- chat_openai(
  model = "gpt-4o",
  system_prompt = 
    "Generate tabular data based on the user's request. Limit the datat to 10 rows unless the user specifically requests more."
)
```

With ellmer, you specify the _provider_ by choosing a `chat_*()` function. LLM providers typically offer multiple _models_, and you can specify your desired model with the `model` argument. Here, we're using gpt-4o. 

Specify the model with the `model` argument. Because we want to use this chat to generate data, we'll also set a system prompt specifying that the LLM should generate tabular data based on the criteria given by the user. 

::: callout-tip
It's best practice to write system prompts in a markdown file. Later, we'll write a longer systtem prompt and do jsut that, but for now we'll just specify it directly to the `system_prompt` argument. For more information about prompt design, see https://ellmer.tidyverse.org/articles/prompt-design.html.
:::

We don't need to manually specify the API key because dotenv will load our environment variables and `chat_openai()` will look for a variable called `OPENAI_API_KEY`. 

Next, we start off the chat by telling the LLM to ask the user about the data they want to generate, and then use `live_console()` to continue the chat in the console.  

```{r}
library(ellmer)
library(dotenv)

chat <- chat_openai(
  model = "gpt-4o",
  system_prompt = 
    "Generate tabular data based on the user's request. Limit the datat to 10 rows unless the user specifically requests more."
)

chat$chat("Ask what kind of data the user wants to generate.", echo = TRUE) 
live_console(chat, quiet = TRUE) 
```

[`chat()`](https://ellmer.tidyverse.org/reference/Chat.html#method-Chat-chat) is a method of a `Chat` object that sends input to the chatbot. Here, our `Chat` object is called `chat`, so we call `chat$chat()`. Here, we tell the chatbot to ask what kind of data we want to generate. This means that when we start the console converstaion with `live_console()`, the chatbot will ask the user  what kind of data they want. 

Let's try it out. 

## Parse the data

[put in screencast]

We've gotten the LLM to simulate data for us, but it's not really in a convenient format. If we wanted to pull this data into R, we would need to parse the string output from the LLM. Let's see if we can improve our command line LLM tool. 

One way is to adjust the system prompt to specify the format of the data and then read that data into R. We can specify that we want the LLM to provide us data in a CSV format. Let's try that. 

::: callout-tip
One thing to look into if you're trying to _extract_ data instead of creating it from scratch: ellmer also includes tools for extracting structured data from text or images https://ellmer.tidyverse.org/articles/structured-data.html.
:::

To do so, we're going to need to expand the system prompt. Let's move the system prompt into its own Markdown file.

```markdown
Generate tabular data based on the user's request. Limit the datat to 10 rows unless the user specifically requests more.

Generate the data in a format that is readable by the R function `readr::read_csv()`. 

Do not include any other information with the dataset. 
```

::: callout-tip
Adding examples to system prompts is very helpful. 
:::

```r
library(ellmer)
library(dotenv)

chat <- chat_openai(
  model = "gpt-4o",
  system_prompt = readr::read_lines("prompt.md")
)

chat$chat("Ask what kind of data the user wants to generate.", echo = TRUE) 
live_console(chat, quiet = TRUE) 
```

```{r}
library(ellmer)
library(dotenv)
library(readr)

chat <- chat_openai(
  model = "gpt-4o",
  system_prompt = read_lines("prompt.md")
)

csv_string <- chat$chat("cat toy data", echo = FALSE) 
df <- read_csv(csv_string, show_col_types = FALSE)
df
```

Now, let's turn this into a function. 

```{r}
generate_data <- function(data_description) {
    chat <- chat_openai(
        model = "gpt-4o",
        system_prompt = read_lines("prompt.md")
    )

    csv_string <- chat$chat(data_description, echo = FALSE) 

    read_csv(csv_string, show_col_types = FALSE)
}
```

Now, we can pass in the description to the function to generate a dataset, without chatting with the chatbot in the console. This will make it easier to use the function in a Shiny app, or 
to quickly generate datasets. 

## Tool calling

One issue is there's no consequence if the LLM decides to generate data that doesn't match our criteria. `read_csv()` will fail, and then we'll need to run the function again (or adjust our prompt) until we're successful.

We can use _tool calling_ to improve this process. Generally, you use tool calling to extend the functionality of an LLM. For example, LLM models generally don't know the time at your location, but we can write an R function that returns that information. Then, we tell the LLM to call that R function and use the result in some way, essentially granting the LLM new skills. 

Here, the LLM can already generate data, but it can't actually read that data into R (because our chatbot can write R code, but can't run R code). 

For more information about tool calling, see the [Tool Calling article](https://ellmer.tidyverse.org/articles/tool-calling.html) on the ellmer website. 

## Define a tool function

First, we need to write the function that takes a string representing CSV data and reads it in R. We will also need to document the function with [roxygen2 comments ](https://roxygen2.r-lib.org/). This will help the LLM understand how to use our function, just like it would help a human use your function. 

```{r}
#' Reads a CSV string into R using `readr::read_csv()`
#'
#' @param csv_string A single string representing literal data in CSV format, able to be read by `readr::read_csv()`.
#' @return A tibble. 
read_csv_string <- function(csv_string) {
  read_csv(csv_string, show_col_types = FALSE)
}
```

There's not much going on in our function, but that's ok!

## Register the tool

Now, we need to tell the LLM about our function. We do this by _registering_ the tool using the `Chat` method `register_tool()` and the function `tool()`. You don't need to write this code yourself. Instead, you can use `ellmer::create_tool_def()` to generate the `register_tool()` call. For example:

```{r}
create_tool_def(read_csv_string)
```

````
```r
tool(
  read_csv_string,
  "Reads a CSV string into R using `readr::read_csv()`.",
  csv_string = type_string(
    "A single string representing literal data in CSV format, able to be read by `readr::read_csv()`."
  )
)
```
````

Now, we can copy-and-paste that code into `chat$register_tool()`. This tells the LLM about our function and explains what kind of arguments it should pass to the function. To learn more about defining a tool with `tool()`, see https://ellmer.tidyverse.org/reference/tool.html. 

```{r}
chat$register_tool(
  tool(
    read_csv_string,
    "Parses a string containing CSV formatted data and reads it into a data frame.",
    csv_string = type_string(
      "CSV string containing the data to be read."
    )
  )
)
```

## Update your prompt 

The final step is to update our prompt to instruct the LLM when to use the tool. Because the usage of our tool is pretty simple, we could probably get away without this step, but it's good practice and will make it more likely that the LLM behaves as we want it to.

```markdown
Generate tabular data based on the user's request. Limit the datat to 10 rows unless the user specifically requests more.

Create a string of data in a format that is readable by the R function `readr::read_csv()`. For example:

TransactionID, Make, Model, Year, Price, Salesperson
001, Toyota, Camry, 2020, 24000, John Smith
002, Honda, Accord, 2019, 22000, Jane Doe
003, Ford, Explorer, 2021, 32000, Emily Johnson
004, Chevrolet, Malibu, 2020, 23000, Michael Brown
005, Nissan, Altima, 2021, 25000, Sarah Davis
006, Hyundai, Elantra, 2019, 19000, David Wilson
007, BMW, X3, 2020, 42000, Mary White
008, Audi, A4, 2021, 38000, Robert Martinez
009, Subaru, Outback, 2020, 27000, James Lee
010, Kia, Soul, 2021, 21000, Linda Thompson

Do not include any text formatting the data (e.g., no backticks). Do not include any other information with the dataset. 

Then, pass that string of data to the function `read_csv_string()`. This function will use `readr::read_csv()` to read the string into R. The function returns a tibble.   

Treat a tibble response from `read_csv_string()` as a success. If the function throws an error, treat that as a failure and re-call the function with a different string of data.  
```

Alone, however, this script isn't very helpful. The LLM generates data, but it doesn't get saved anywhere! And `chat$chat()` returns as a string, so if we ask the LLM to return our table we're going to run into the same issue as before: we need a way to read it into R.

To allow the data to persist, we could write it to a file or database. Another option for this functionality is to embed it in a Shiny app that lets the user generate data and then visualize it. In that case, we could update a reactive value to the generated data. 

You can see that app here--it also includes a tool call that plots the data. 

## Another option

Even though 

Use `create_tool_def(read_csv_string)` to create the register tool code

Let's build a command-line tool that simulates data for you. text

# Step 1 - setup
# Install ellmer and dotenv
# Add an API key for your LLM 

# Step 2 - create a basic chat with elmer
# we're going to outline how our script will work and then move back to the prompt
# first, we're going to just create a chat with ellmer and start the chat

# Step 3 - get ellmer to generate data
# this is actually pretty straightforward
# it can generate data for us
# and if you’re using a chat interface, you can even download it as a csv
# but what if you want this functionality in a shiny app, or you want to immediately use the data in r or python? 
# also, this isn’t very reproducible. It’s just a string


If you know the column names and types of data you want to generate, you could also use structured data. 